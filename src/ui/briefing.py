import streamlit as st

from PIL import Image

from datetime import datetime, timezone
from config import EXAMPLE_IMAGES


@st.cache_data
def display_briefing() -> None:
    st.title('Survey Instructions')

    st.markdown("""
    In this study, you will help us evaluate **visual explanations** generated by machine learning (ML) models.

    Modern ML models are often applied to image classification tasks. For instance, identifying whether an image shows a dog, a cat, or a car. While these models are often accurate, understanding *why* a model made a certain prediction is critical for building trust and improving transparency.

    That's where **explanation methods** come into play. These methods highlight image regions that were most important for the model's decision. In this survey, your task is to assess how well these highlighted regions align with the actual object in the image.

    **Goal**: We want to understand how well explanations align with the **ground truth** (i.e., the real object in the image) and if explanations also highlight something other than the **ground truth**, if that is helpful for understanding its decision. 
    
    Your responses provide a human perspective that goes beyond what automated metrics can capture. 
    
    **Important:** Across all samples you will be presented, we assume, that the model is not tricked by artifacts or other features that are not related to the **ground truth** object. So this is nothing you need to worry about, but rather a note for you to understand the task better.

    ---
    """)

    st.subheader('How to Rate Explanations')

    st.write("""
    For each explanation, you will see an image with a **green border** around the classified object - the **ground truth**. Next to it, you will find an **importance map** that highlights the regions the model considered most influential for its classification.
    you will rate each explanation (79 in total) on two scales:
    
    **Alignment with Ground Truth**: To what degree does the importance map align with the classified object bordered in green?
    In simpler terms, does the highlighted region give you the impression, that the model is focusing on the **ground truth**?
    
    **Relevance of Explanation**: Does the importance map not related to the **ground truth** appear meaningful for classifying the image?
    In simpler terms, if the explanation highlights parts of the image that are not related to the **ground truth** object, does it still provide useful information for understanding the model's decision (i.e., observing a street helps to classify an object as a car, making it relevant)?
    Both questions use a scale from **1 (poor) to 5 (perfect)**.
    For the second question, you can also select 'The importance map is perfectly aligned' if you perceive the explanation to be entirely focused on the **ground truth**.
    
    **Note: you can not skip back to previous questions.** Once you proceed to the next question, your answer is final."""
             )

    st.subheader('Example Evaluations')
    """
    In the following examples, you will see multiple different explanations for two images, their corresponding ground truth, and some comments from our side.
    We want them to give you the following intuition:
    The shape/ style of different importance maps depends heavily on the method.
    As a consequence, an ideal importance map looks by nature different for each method. 
    There are cases, where two methods can perfectly explain the model, yet their importance maps look very different in shape/ style.
    
    Make yourself familiar with the different methods and their resulting explanation shape/ style by looking at the examples below. Afterwards, proceed to the survey."""

    # Add a divider for better separation
    st.divider()

    # Display the examples
    for _, images in EXAMPLE_IMAGES.items():
        for exp_path, comment in images:
            st.image(Image.open(exp_path), use_container_width=True)
            st.write(f'{comment}')

        # Add a divider for better separation
        st.divider()

    st.markdown("""
    **General things to keep in mind:**
    
    The scope of this survey is to evaluate the perceptual alignment of explanations with the ground truth object. Ask yourself if you perceive the explanation is relevant for your understanding of the model's decision.
    Here are some examples, that might help to understand what this means better:
    - Relevance maps are highlighting regions in 2D space. If e.g. an object is in the background of the image, covered by a see-through object like a fence, simply ask yourself, if your attention is drawn to the **ground truth** object, or if you are confused by the foreground object (e.g. fence). If you are distracted, the explanation may not be perfectly aligned with the **ground truth** and vice versa
    - Sometimes the relevance map only highlights parts of the **ground truth** object. Again, the score you assign depends on how well the explanation leads your attention to the **ground truth** or not, and not strictly on how well the explanation covers the **ground truth** object
    """)


def store_briefing() -> None:
    """
    Store the user's briefing response in the database
    """
    st.session_state.examples_shown = True
    st.session_state.db['briefings'].insert_one(
        {
            'pid':
            st.session_state.prolific_pid,
            'user_group':
            st.session_state.username,
            'user_id':
            st.session_state.user_id,
            'start':
            st.session_state.timestamp,
            'end':
            datetime.now(timezone.utc).isoformat()
        }
    )
    st.rerun()
