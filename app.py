import datetime
import json
import os
import random
import streamlit as st
import uuid

from collections import defaultdict
from datetime import datetime, timedelta
from PIL import Image

from config import IMAGE_FOLDER, NUM_SAMPLES, QUESTION_SCALE_MAP, EXAMPLE_IMAGES, NUM_CHECKS, ATTENTION_CHECKS, RESERVATION_TIMEOUT
from get_database import get_database
from permuation import populate_samples

def increase_font_size() -> None:
    """
    Increases the font size of radio titles in the Streamlit app.
    """
    st.markdown(
            """
            <style>
            div[class*="stRadio"] > label > div[data-testid="stMarkdownContainer"] > p {
            font-size: 18px;
            }
            </style>
            """, 
            unsafe_allow_html=True)
        

# Login function
def login(username, password) -> bool:
    user = st.session_state.db['users'].find_one({'username': username})
    if user and user['password'] == password:
        return True
    return False


def draw_samples(num_samples, session_state) -> list:
    remaining_samples = session_state.db['combinations']
    now = datetime.now()
    
    def get_available_samples():
        return list(remaining_samples.find({
            '$or': [
                {'reserved_until': {'$exists': False}}, 
                {'reserved_until': {'$lt': now.isoformat()}}
            ]
        }))
    
    all_samples = get_available_samples()
    if len(all_samples) == 0:
        populate_samples()
        all_samples = get_available_samples()  # Try again
        
    grouped_by_folder = defaultdict(list)
    for sample in all_samples:
        grouped_by_folder[sample['sample']].append(sample)

    folders = list(grouped_by_folder.keys())
    random.shuffle(folders)
    drawn_samples = []
    
    if len(folders) < NUM_SAMPLES:
        populate_samples() 
        
    for folder in folders:
        if len(drawn_samples) >= num_samples:
            break
        
        sample = random.choice(grouped_by_folder[folder])
        reservation_expiry = now + timedelta(seconds=RESERVATION_TIMEOUT)
        result = remaining_samples.update_one(
            {
                '_id': sample['_id'],
                '$or': [
                    {'reserved_until': {'$exists': False}},
                    {'reserved_until': {'$lt': now.isoformat()}}
                ]
            },
            {'$set': {'reserved_until': reservation_expiry.isoformat()}}
        )
        
        if result.modified_count == 1:
            # reservation successful, add to drawn samples
            drawn_samples.append(sample)
        else:
            # reservation not successful, skip this sample
            continue
        
    assert len(drawn_samples) == NUM_SAMPLES
    print(f'Sampled {len(drawn_samples)} explanations')
    return drawn_samples


@st.cache_data
def display_briefing() -> None:
    st.title('Survey Instructions')
    
    st.markdown("""
    In this study, you will help us evaluate **visual explanations** generated by machine learning (ML) models.

    Modern ML models are often applied to image classification tasks. For instance, identifying whether an image shows a dog, a cat, or a car. While these models are often accurate, understanding *why* a model made a certain prediction is critical for building trust and improving transparency.

    That's where **explanation methods** come into play. These methods highlight image regions that were most important for the model's decision. In this survey, your task is to assess how well these highlighted regions align with the actual object in the image.

    **Goal**: We want to understand how well explanations align with the **ground truth** (i.e., the real object in the image) and if explanations also highlight something other than the **ground truth**, if that is helpful for understanding its decision. 
    
    Your responses provide a human perspective that goes beyond what automated metrics can capture. 
    
    **Important:** Across all samples you will be presented, we assume, that the model is not tricked by artifacts or other features that are not related to the **ground truth** object. So this is nothing you need to worry about, but rather a note for you to understand the task better.

    ---
    """)

    st.subheader('How to Rate Explanations')

    st.write("""
    For each explanation, you will see an image with a **green border** around the classified object - the **ground truth**. Next to it, you will find an **importance map** that highlights the regions the model considered most influential for its classification.
    you will rate each explanation (75 in total) on two scales:
    
    **Alignment with Ground Truth**: To what degree does the importance map align with the classified object bordered in green?
    In simpler terms, does the highlighted region give you the impression, that the model is focusing on the **ground truth**?
    
    **Relevance of Explanation**: Does the importance map not related to the **ground truth** appear meaningful for classifying the image?
    In simpler terms, if the explanation highlights parts of the image that are not related to the **ground truth** object, does it still provide useful information for understanding the model's decision (i.e., observing a street helps to classify an object as a car, making it relevant)?
    Both questions use a scale from **1 (poor) to 5 (perfect)**.
    For the second question, you can also select 'The importance map is perfectly aligned' if you perceive the explanation to be entirely focused on the **ground truth**.
    
    **Note: you can not skip back to previous questions.** Once you proceed to the next question, your answer is final.""")

    st.subheader('Example Evaluations')
    
    """
    In the following examples, you will see multiple different explanations for two images, their corresponding ground truth, and some comments from our side.
    We want them to give you the following intuition:
    The shape/ style of different importance maps depends heavily on the method.
    As a consequence, an ideal importance map looks by nature different for each method. 
    There are cases, where two methods can perfecty explain the model, yet their importance maps look very different in shape/ style.
    
    Make yourself familiar with the different methods and their resulting explanation shape/ style by looking at the examples below. Afterwards, proceed to the survey."""
    
    st.divider() # Add a divider for better separation
    
    # Display the examples
    for _, images in EXAMPLE_IMAGES.items():
        for exp_path, comment in images:
            st.image(Image.open(exp_path), use_container_width=True)
            st.write(f'{comment}')
        st.divider() # Add a divider for better separation
    
    """
    **General things to keep in mind:**
    
    The scope of this survey is to evaluate the perceptual alignment of explanations with the ground truth object. Ask yourself, if you perceive the explanation is relevant for your understanding of the model's decision.
    Here are some examples, that might help to understand what this means better:
    - Relevance maps are highlighting regions in 2D space. If e.g. an object is in the background of the image, covered by a see-through object like a fence, simply ask yourself, if your attention is drawn to the **ground truth** object, or if you are confused by the forground object (e.g. fence). If you are distracted, the explanation may not be perfectly aligned with the **ground truth** and vice versa
    - Sometimes the relevance map only highlights parts of the **ground truth** object. Again, the score you assign depends on how well the explanation leads your attention to the **ground truth** or not, and not strictly on how well the explanation covers the **ground truth** object
    """
    
    st.divider() 
    
    
# Initialize session state variables
if 'logged_in' not in st.session_state:
    st.session_state.logged_in = False 
if 'evaluation_started' not in st.session_state:
    st.session_state.evaluation_started = False
if 'examples_shown' not in st.session_state:
    st.session_state.examples_shown = False
if 'current_index' not in st.session_state:
    st.session_state.current_index = 0
if 'current_sample_count' not in st.session_state:
    st.session_state.current_sample_count = 1
if 'manipulation_checks' not in st.session_state:
    st.session_state.manipulation_checks = []
if 'ml_familiarity' not in st.session_state:
    st.session_state.ml_familiarity = None
if 'show_warning' not in st.session_state:
    st.session_state.show_warning = False  # Flag for warning visibility
if 'db' not in st.session_state:
        st.session_state.db = get_database()
if 'user_id' not in st.session_state:
        while True:
            user_id = str(uuid.uuid4())  # Generate a new UUID
            if not st.session_state.db['responses'].find_one({'user_id': user_id}):  # Check if the user ID already exists
                st.session_state.user_id = user_id
                break
            
            
if not st.session_state.logged_in:
    st.title('Login')  # Login UI
    st.session_state.username = st.text_input('Username')
    st.session_state.password = st.text_input('Password', type='password')
    if st.button('Login'):
        if login(st.session_state.username, st.session_state.password):  # Validate credentials
            st.session_state.logged_in = True
            st.success('Login successful!')
            st.rerun()
        else:
            st.error('Invalid username or password.')  # Error message for invalid credentials
        st.stop()
        
# Show examples before starting evaluation
elif not st.session_state.examples_shown:
    if 'timestamp' not in st.session_state:
        st.session_state.timestamp = datetime.now()
        
    display_briefing()
    
    if st.button('Proceed to Survey'):
        st.session_state.examples_shown = True
        st.session_state.db['briefings'].insert_one({
                    'user_group': st.session_state.username,
                    'user_id': st.session_state.user_id,
                    'start': st.session_state.timestamp,
                    'end': datetime.now().isoformat()
                })
        st.rerun()

# Initial question
else:
    if not st.session_state.evaluation_started:
        familiarity_map = QUESTION_SCALE_MAP['familarity']
        
        # Set a default value only if not already set
        if 'ml_familiarity' not in st.session_state:
            st.session_state.ml_familiarity = None

        # Bind radio selection directly to session state
        selected_familiarity = st.radio(
            familiarity_map['question'],
            familiarity_map['scale'], 
            index=None,
            horizontal=True
        )
        
        increase_font_size()
        st.info("After pressing 'Start Evaluation', please wait a few seconds until random sampling of explanations is completed.")
        
        if st.button('Start Evaluation'):
            if selected_familiarity is not None:
                st.session_state.evaluation_started = True
                st.session_state.ml_familiarity = selected_familiarity
                st.session_state.timestamp = datetime.now().isoformat()
                familiarity = {
                    'user_group': st.session_state.username,
                    'user_id': st.session_state.user_id,
                    'ml_familiarity': st.session_state.ml_familiarity,
                    'timestamp': st.session_state.timestamp
                }
                st.session_state.db['familiarities'].insert_one(familiarity)
                st.session_state.show_warning = False
                st.rerun()
            else:
                st.session_state.show_warning = True
        if st.session_state.show_warning:
            st.warning('Please select an answer before proceeding.')
        st.stop()
    
    # Sample explanations if not already sampled
    if 'sampled_explanations' not in st.session_state:
        drawn_samples = draw_samples(NUM_SAMPLES, st.session_state)
        sampled_explanations = []
        for drawn_sample in drawn_samples:
            sample_folder = drawn_sample['sample']
            method = drawn_sample['method']
            threshold = drawn_sample['threshold']
            sampled_explanations.append({
                'type': 'xai',
                'object_folder': sample_folder,
                'method': method,
                'threshold': threshold
            })
            print(f'Sampled: {sample_folder}, {method}, {threshold}')
        
        attention_checks = random.sample(ATTENTION_CHECKS, k=NUM_CHECKS)
        for check in attention_checks:
            insert_index = random.randint(0, len(sampled_explanations))
            sampled_explanations.insert(insert_index, check)
        st.session_state.sampled_explanations = sampled_explanations
    
    if st.session_state.current_index < len(st.session_state.sampled_explanations):
        # Get the current drawn_sample
        drawn_sample = st.session_state.sampled_explanations[st.session_state.current_index]
        current_type = drawn_sample.get('type')
        
        # Count how many XAI samples are left (excluding attention checks)
        if drawn_sample.get('type') != 'xai':
            if drawn_sample.get('type') == 'manipulation':
                st.markdown('**Please indicate your agreement with the statements below**')
                answer = st.radio(
                    drawn_sample['question'],
                    ['Strongly Disagree', 'Disagree', 'Agree', 'Strongly Agree'],
                    index=None,
                    key=f"manipulation_{st.session_state.current_index}",
                    horizontal=True
                )

            elif drawn_sample.get('type') == 'attention':
                st.markdown(f"**{drawn_sample['question']}**")
                answer = st.radio(
                    'Based on the text you read above, what colour have you been asked to enter?',
                    ['Red', 'Blue', 'Green', 'Orange', 'Brown'],
                    index=None,
                    key=f'attention_{st.session_state.current_index}'
                )
            
            if st.button('Next'):
                if answer is None:
                    st.session_state.show_warning = True
                    st.rerun()
                print('check')
                # Save the responses
                manipulation_check = {
                    'user_id': st.session_state.user_id,
                    'question': drawn_sample['question'],
                    'answer': answer,
                    'failed': 0 if answer in drawn_sample.get('correct_answers') else 1,
                    'index': st.session_state.current_index,
                    'timestamp': datetime.now().isoformat()
                }
                
                st.session_state.db['manipulation_checks'].insert_one(manipulation_check)

                st.session_state.manipulation_checks.append(manipulation_check)
                st.session_state.current_index += 1
                st.session_state.show_warning = False  # Reset warning when user proceeds
                st.rerun()
            
            # Display warning if no response is selected
            if st.session_state.show_warning:
                st.warning('Please select an answer to continue.')
        else:
            st.markdown(f'### Sample {st.session_state.current_sample_count} of {NUM_SAMPLES}')
            st.divider() # Add a divider for better separation
            
            object_folder = drawn_sample['object_folder']
            method = drawn_sample['method']
            threshold = drawn_sample['threshold']

            explanation_path = os.path.join(IMAGE_FOLDER, object_folder, method, threshold)
            st.image(Image.open(explanation_path), use_container_width=True)

            st.info("Feel free to use 'cmd +' or 'ctrl +' to zoom in on the image for better visibility.")
            
            st.divider() # Add a divider for better separation
            
            # Ask the alignment question and check for a valid response
            alignment = None
            alignment_map = QUESTION_SCALE_MAP['alignment']
            alignment = st.radio(
                alignment_map['question'],
                alignment_map['scale'],
                index=None, 
                key=f'xai_alignment_{st.session_state.current_index}',
                horizontal=True
            )
            
            st.divider() # Add a divider for better separation
            
            # Ask the relevance question and check for a valid response
            relevance = None
            relevance_map = QUESTION_SCALE_MAP['relevance']
            relevance = st.radio(
                relevance_map['question'],
                relevance_map['scale'],
                index=None, 
                key=f'xai_relevance_{st.session_state.current_index}',
                horizontal=True
            )
            
            increase_font_size()
            st.divider() # Add a divider for better separation
                
            if st.button('Next'):
                if alignment is None or relevance is None:
                    st.session_state.show_warning = True
                    st.rerun()
                print('xai check')
                # Save the responses
                response = {
                    'user_group': st.session_state.username,
                    'user_id': st.session_state.user_id,
                    'sample': object_folder,
                    'method': method,
                    'threshold': threshold.split('.')[0],
                    'alignment': alignment,
                    'relevance': relevance,
                    'timestamp': datetime.now().isoformat()
                }

                st.session_state.db['responses'].insert_one(response)

                # Delete the evaluated drawn_sample from the collection
                st.session_state.db['combinations'].delete_one({
                    'sample': object_folder,
                    'method': method,
                    'threshold': threshold
                })
            
                st.session_state.current_index += 1
                st.session_state.current_sample_count += 1
                st.session_state.show_warning = False  # Reset warning when user proceeds
                st.rerun()
                
            # Display warning if no response is selected
            if st.session_state.show_warning:
                st.warning('Please select answers to continue.')
                
    elif 'self_evaluation' not in st.session_state:
        self_evaluation_map = QUESTION_SCALE_MAP['self_evaluation']

        # Bind radio selection directly to session state
        self_evaluation_response = st.radio(
            self_evaluation_map['question'],
            self_evaluation_map['scale'], 
            index=None,
            horizontal=True
        )
        
        increase_font_size() 
        
        if st.button('Submit Survey'):
            if self_evaluation_response is not None:
                st.session_state.timestamp = datetime.now().isoformat()
                self_evaluation = {
                    'user_group': st.session_state.username,
                    'user_id': st.session_state.user_id,
                    'self_evaluation': self_evaluation_response,
                    'timestamp': st.session_state.timestamp
                }
                st.session_state.self_evaluation = self_evaluation  # Store in session state for rerun
                st.session_state.db['self_evaluations'].insert_one(self_evaluation)
                st.session_state.show_warning = False
                st.rerun()
            else:
                st.session_state.show_warning = True
        # Display warning if no response is selected
        if st.session_state.show_warning:
            st.warning('Please select an answer before submitting.')
    else:
        number_checks_failed = 0
        for manipulation_check in st.session_state.manipulation_checks:
            # Insert each manipulation check into the database
            number_checks_failed += manipulation_check.get('failed')
        
        st.session_state.db['manipulation_reports'].insert_one({
            'user_group': st.session_state.username,
            'user_id': st.session_state.user_id,
            'indices': [mc['index'] for mc in st.session_state.manipulation_checks],
            'number_checks': len(st.session_state.manipulation_checks),
            'number_checks_failed': number_checks_failed
        })
        
        st.success('Evaluation completed! Results sent to MongoDB.')