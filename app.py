import datetime
import json
import os
import random
import streamlit as st
import uuid

from collections import defaultdict
from datetime import datetime, timedelta
from PIL import Image

from config import IMAGE_FOLDER, NUM_SAMPLES, QUESTION_SCALE_MAP, EXAMPLE_IMAGES, NUM_CHECKS, ATTENTION_CHECKS, RESERVATION_TIMEOUT
from get_database import get_database
from permuation import populate_samples

def increase_font_size() -> None:
    """
    Increases the font size of radio titles in the Streamlit app.
    """
    st.markdown(
            """
            <style>
            div[class*="stRadio"] > label > div[data-testid="stMarkdownContainer"] > p {
            font-size: 18px;
            }
            </style>
            """, 
            unsafe_allow_html=True)
        

# Login function
def login(username, password) -> bool:
    user = st.session_state.db['users'].find_one({"username": username})
    if user and user['password'] == password:
        return True
    return False


def draw_samples(num_samples, remaining_samples) -> list:
    now = datetime.now()
    
    available_samples_cursor = remaining_samples.find({
        '$or': [
            {'reserved_until': {'$exists': False}}, #not reserved
            {'reserved_until': {'$lt': now.isoformat()}} #reserved but expired
        ]
    })
    
    all_samples = list(available_samples_cursor)
    grouped_by_folder = defaultdict(list)

    for sample in all_samples:
        grouped_by_folder[sample['sample']].append(sample)

    folders = list(grouped_by_folder.keys())
    random.shuffle(folders)

    drawn_samples = []    
    for folder in folders:
        if len(drawn_samples) >= num_samples:
            break
        sample = random.choice(grouped_by_folder[folder])
        reservation_expiry = now + timedelta(seconds=RESERVATION_TIMEOUT)
        result = remaining_samples.update_one(
            {
                '_id': sample['_id'],
                '$or': [
                    {'reserved_until': {'$exists': False}},
                    {'reserved_until': {'$lt': now}}
                ]
            },
            {'$set': {'reserved_until': reservation_expiry.isoformat()}}
        )
        
        if result.modified_count == 1:
            # reservation successful, add to drawn samples
            drawn_samples.append(sample)
        else:
            # reservation not successful, skip this sample
            continue

    return drawn_samples


@st.cache_data
def display_briefing() -> None:
    st.title("Explanation Evaluation: Survey Instructions")
    
    st.markdown("""
    In this study, You will help us evaluate **visual explanations** generated by machine learning (ML) models.

    Modern ML models are often applied to image classification tasks. For instance, identifying whether an image shows a dog, a cat, or a car. While these models are often accurate, understanding *why* a model made a certain prediction is critical for building trust and improving transparency.

    That's where **explanation methods** come into play. These methods highlight image regions that were most important for the model's decision. In this survey, Your task is to assess how well these highlighted regions align with the actual object in the image.

    **Goal**: We want to understand how well explanations align with the **ground truth** (i.e., the real object in the image) and if explanations also highlight sotmethin other than the **ground truth**, if that is helpful for understanding its decision. 
    
    Your responses provide a human perspective that goes beyond what automated metrics can capture.

    ---
    """)

    st.subheader("How to Rate Explanations")

    st.write("""
    For each explanation, You will see an image with a **green border** around the classified object - the **ground truth**. Next to it, You will find an **importance map** that highlights the regions the model considered important for its classification.
    You will rate each explanation on two scales:
    
    **Alignment with Ground Truth**: To what degree does the importance map align with the classified object bordered in green?
    In simpler terms, does the highlighted region give You the impression, that the model is focusing on the **ground truth**?
    
    **Relevance of Explanation**: Does the importance map not related to the **ground truth** appear meaningful for classifying the image?
    In simpler terms, if the explanation highlights parts of the image that are not related to the **ground truth** object, does it still provide useful information for understanding the model's decision?
    Both questions use a scale from **1 (poor) to 5 (perfect)**.
    For the second question, You can also select "The importance map is perfectly aligned" if You perceive the explanation to be entirely focused on the **ground truth**.
    """)

    st.subheader("Example Evaluations")
    
    """
    In the following examples, You will see multiple different explanations for two images, their corresponding ground truth, and some comments from our side.
    We want them to give You the following intuition:
    The shape/ style of different importance maps depends heavily on the method.
    As a consequence, an ideal importance map looks by nature different for each method. 
    There are cases, where two methods can perfecty explain the model, yet their importance maps look very different in shape/ style.
    
    Make Yourself familiar with different methods and their resulting explanation shape/ style by looking at the examples below. Afterwards, proceed to the survey."""
    
    st.divider() # Add a divider for better separation
    
    # Display the examples
    for _, images in EXAMPLE_IMAGES.items():
        for exp_path, comment in images:
            st.image(Image.open(exp_path), use_container_width=True)
            st.write(f'{comment}')
        st.divider() # Add a divider for better separation

# Initialize session state variables
if 'logged_in' not in st.session_state:
    st.session_state.logged_in = False 
if 'evaluation_started' not in st.session_state:
    st.session_state.evaluation_started = False
if 'examples_shown' not in st.session_state:
    st.session_state.examples_shown = False
if 'current_index' not in st.session_state:
    st.session_state.current_index = 0
if 'manipulation_checks' not in st.session_state:
    st.session_state.manipulation_checks = []
if 'ml_familiarity' not in st.session_state:
    st.session_state.ml_familiarity = None
if 'show_warning' not in st.session_state:
    st.session_state.show_warning = False  # Flag for warning visibility
if 'db' not in st.session_state:
        st.session_state.db = get_database()
if 'user_id' not in st.session_state:
        while True:
            user_id = str(uuid.uuid4())  # Generate a new UUID
            if not st.session_state.db['responses'].find_one({'user_id': user_id}):  # Check if the user ID already exists
                st.session_state.user_id = user_id
                break
            
            
if not st.session_state.logged_in:
    st.title("Login")  # Login UI
    st.session_state.username = st.text_input("Username")
    st.session_state.password = st.text_input("Password", type="password")
    if st.button("Login"):
        if login(st.session_state.username, st.session_state.password):  # Validate credentials
            st.session_state.logged_in = True
            st.success("Login successful!")
            st.rerun()
        else:
            st.error("Invalid username or password.")  # Error message for invalid credentials
        st.stop()
        
# Show examples before starting evaluation
elif not st.session_state.examples_shown:
    if 'timestamp' not in st.session_state:
        st.session_state.timestamp = datetime.now()
        
    display_briefing()
    
    if st.button('Proceed to Survey'):
        st.session_state.examples_shown = True
        st.session_state.db['briefings'].insert_one({
                    'user_group': st.session_state.username,
                    'user_id': st.session_state.user_id,
                    'start': st.session_state.timestamp,
                    'end': datetime.now().isoformat()
                })
        st.rerun()

# Initial question
else:
    if not st.session_state.evaluation_started:
        familiarity_map = QUESTION_SCALE_MAP['familarity']
        
        # Set a default value only if not already set
        if 'ml_familiarity' not in st.session_state:
            st.session_state.ml_familiarity = None

        # Bind radio selection directly to session state
        selected_familiarity = st.radio(
            familiarity_map['question'],
            familiarity_map['scale'], 
            index=None,
            horizontal=True
        )
        
        increase_font_size() 
        
        if st.button('Start Evaluation'):
            if selected_familiarity is not None:
                st.session_state.evaluation_started = True
                st.session_state.ml_familiarity = selected_familiarity
                st.session_state.timestamp = datetime.now().isoformat()
                familiarity = {
                    'user_group': st.session_state.username,
                    'user_id': st.session_state.user_id,
                    'ml_familiarity': st.session_state.ml_familiarity,
                    'timestamp': st.session_state.timestamp
                }
                st.session_state.db['familiarities'].insert_one(familiarity)
                st.session_state.show_warning = False
                st.rerun()
            else:
                st.session_state.show_warning = True
        if st.session_state.show_warning:
            st.warning('Please select an answer before proceeding.')
        st.stop()
    
    # Sample explanations if not already sampled
    if 'sampled_explanations' not in st.session_state:
        drawn_samples = draw_samples(NUM_SAMPLES, st.session_state.db['combinations'])
        sampled_explanations = []
        for drawn_sample in drawn_samples:
            sample_folder = drawn_sample['sample']
            method = drawn_sample['method']
            threshold = drawn_sample['threshold']
            sampled_explanations.append({
                'type': 'xai',
                'object_folder': sample_folder,
                'method': method,
                'threshold': threshold
            })
            print(f"Sampled: {sample_folder}, {method}, {threshold}")
        
        attention_checks = random.sample(ATTENTION_CHECKS, k=NUM_CHECKS)
        for check in attention_checks:
            insert_index = random.randint(0, len(sampled_explanations))
            sampled_explanations.insert(insert_index, check)
        st.session_state.sampled_explanations = sampled_explanations
        
    if st.session_state.current_index < len(st.session_state.sampled_explanations):
        # Get the current drawn_sample
        drawn_sample = st.session_state.sampled_explanations[st.session_state.current_index]
        
        if drawn_sample.get('type') != 'xai':
            if drawn_sample.get('type') == 'manipulation':
                st.markdown("**Please indicate your agreement with the statements below**")
                answer = st.radio(
                    drawn_sample['question'],
                    ["Strongly Disagree", "Disagree", "Agree", "Strongly Agree"],
                    index=None,
                    key=f"manipulation_{st.session_state.current_index}",
                    horizontal=True
                )

            elif drawn_sample.get('type') == 'attention':
                st.markdown(f"**{drawn_sample['question']}**")
                answer = st.radio(
                    "Based on the text you read above, what colour have you been asked to enter?",
                    ['Red', 'Blue', 'Green', 'Orange', 'Brown'],
                    index=None,
                    key=f"attention_{st.session_state.current_index}"
                )
                
            if st.button('Next'):
                if answer is None:
                    st.session_state.show_warning = True
                    st.rerun()
                # Save the responses
                manipulation_check = {
                    'user_id': st.session_state.user_id,
                    'question': drawn_sample['question'],
                    'answer': answer,
                    'failed': 0 if answer in drawn_sample.get('correct_answers') else 1,
                    'index': st.session_state.current_index,
                    'timestamp': datetime.now().isoformat()
                }
                
                st.session_state.db['manipulation_checks'].insert_one(manipulation_check)

                st.session_state.manipulation_checks.append(manipulation_check)
                st.session_state.current_index += 1
                st.session_state.show_warning = False  # Reset warning when user proceeds
                st.rerun()
            
            # Display warning if no response is selected
            if st.session_state.show_warning:
                st.warning('Please select an answer to continue.')
        else:
            object_folder = drawn_sample['object_folder']
            method = drawn_sample['method']
            threshold = drawn_sample['threshold']

            explanation_path = os.path.join(IMAGE_FOLDER, object_folder, method, threshold)
            st.image(Image.open(explanation_path), use_container_width=True)

            st.divider() # Add a divider for better separation
            
            # Ask the alignment question and check for a valid response
            alignment = None
            alignment_map = QUESTION_SCALE_MAP['alignment']
            alignment = st.radio(
                alignment_map['question'],
                alignment_map['scale'],
                index=None, 
                key=f'xai_alignment_{st.session_state.current_index}',
                horizontal=True
            )
            
            st.divider() # Add a divider for better separation
            
            # Ask the relevance question and check for a valid response
            relevance = None
            relevance_map = QUESTION_SCALE_MAP['relevance']
            relevance = st.radio(
                relevance_map['question'],
                relevance_map['scale'],
                index=None, 
                key=f'xai_relevance_{st.session_state.current_index}',
                horizontal=True
            )
            
            increase_font_size()
            st.divider() # Add a divider for better separation
                
            if st.button('Next'):
                if alignment is None or relevance is None:
                    st.session_state.show_warning = True
                    st.rerun()
                # Save the responses
                response = {
                    'user_group': st.session_state.username,
                    'user_id': st.session_state.user_id,
                    'sample': object_folder,
                    'method': method,
                    'threshold': threshold.split('.')[0],
                    'alignment': alignment,
                    'relevance': relevance,
                    'timestamp': datetime.now().isoformat()
                }

                st.session_state.db['responses'].insert_one(response)

                # Delete the evaluated drawn_sample from the collection
                st.session_state.db['combinations'].delete_one({
                    'sample': object_folder,
                    'method': method,
                    'threshold': threshold
                })
            
                st.session_state.current_index += 1
                st.session_state.show_warning = False  # Reset warning when user proceeds
                st.rerun()
                
            # Display warning if no response is selected
            if st.session_state.show_warning:
                st.warning('Please select answers to continue.')
                
    elif 'self_evaluation' not in st.session_state:
        self_evaluation_map = QUESTION_SCALE_MAP['self_evaluation']

        # Bind radio selection directly to session state
        self_evaluation_response = st.radio(
            self_evaluation_map['question'],
            self_evaluation_map['scale'], 
            index=None,
            horizontal=True
        )
        
        increase_font_size() 
        
        if st.button('Submit Survey'):
            if self_evaluation_response is not None:
                st.session_state.timestamp = datetime.now().isoformat()
                self_evaluation = {
                    'user_group': st.session_state.username,
                    'user_id': st.session_state.user_id,
                    'self_evaluation': self_evaluation_response,
                    'timestamp': st.session_state.timestamp
                }
                st.session_state.self_evaluation = self_evaluation  # Store in session state for rerun
                st.session_state.db['self_evaluations'].insert_one(self_evaluation)
                st.session_state.show_warning = False
                st.rerun()
            else:
                st.session_state.show_warning = True
        # Display warning if no response is selected
        if st.session_state.show_warning:
            st.warning('Please select an answer before submitting.')
    else:
        number_checks_failed = 0
        for manipulation_check in st.session_state.manipulation_checks:
            # Insert each manipulation check into the database
            number_checks_failed += manipulation_check.get('failed')
        
        st.session_state.db['manipulation_reports'].insert_one({
            'user_group': st.session_state.username,
            'user_id': st.session_state.user_id,
            'indices': [mc['index'] for mc in st.session_state.manipulation_checks],
            'number_checks': len(st.session_state.manipulation_checks),
            'number_checks_failed': number_checks_failed
        })
        
        st.success('Evaluation completed! Results sent to MongoDB.')