import datetime
import json
import os
import random
import streamlit as st
import uuid

from PIL import Image
from datetime import datetime

from config import IMAGE_FOLDER, NUM_SAMPLES, QUESTION_SCALE_MAP, EXAMPLE_IMAGES, NUM_CHECKS, ATTENTION_CHECKS
from get_database import get_database
from permuation import populate_samples

def increase_font_size() -> None:
    """
    Increases the font size of radio titles in the Streamlit app.
    """
    st.markdown(
            """
            <style>
            div[class*="stRadio"] > label > div[data-testid="stMarkdownContainer"] > p {
            font-size: 18px;
            }
            </style>
            """, 
            unsafe_allow_html=True)
        

# Login function
def login(username, password) -> bool:
    user = users_collection.find_one({"username": username})
    if user and user['password']==password:
        return True
    return False


def draw_samples(num_samples, remaining_samples) -> list:
    """
    samples unique object folders and selects one method and threshold image for each.
    Ensures that folders with no remaining drawn_samples are skipped.
    Repopulates the collection if all drawn_samples are exhausted.
    """
    drawn_samples = []

    while len(drawn_samples) < num_samples:
        # Check if there are any unused drawn_samples
        if remaining_samples.count_documents({}) == 0:
            # If no drawn_samples are left, repopulate the collection
            populate_samples()

        # Get all unique object folders with remaining drawn_samples
        unique_folders = remaining_samples.distinct('sample')

        # Filter out folders that have no remaining drawn_samples
        valid_folders = [
            folder for folder in unique_folders
            if remaining_samples.count_documents({'sample': folder}) > 0
        ]

        # If there are no valid folders left after repopulation, break the loop
        if not valid_folders:
            print("No valid folders left to sample from, even after repopulation.")
            break

        # Randomly select up to the remaining required remaining_samples from valid folders
        remaining_num_samples = num_samples - len(drawn_samples)
        sampled_folders = random.sample(valid_folders, min(remaining_num_samples, len(valid_folders)))

        # For each folder, randomly select one method and threshold image
        for folder in sampled_folders:
            # Get all methods for the folder
            methods = remaining_samples.distinct('method', {'sample': folder})

            # Randomly select a method
            selected_method = random.choice(methods)

            # Get all thresholds for the selected folder and method
            #thresholds = remaining_samples.distinct('threshold', {'sample': folder, 'method': selected_method})

            # Randomly select a threshold
            #selected_threshold = random.choice(thresholds)

            # Find the specific drawn_sample and mark it as reserved
            drawn_sample = remaining_samples.find_one_and_update(
                {'sample': folder, 'method': selected_method, 'threshold': '0.jpg', 'reserved': {'$ne': True}},
                {'$set': {'reserved': True}},  # Mark as reserved
                sort=[('_id', 1)],  # Sort to ensure deterministic order
                return_document=True  # Return the updated document
            )
            if drawn_sample:
                drawn_samples.append(drawn_sample)

    return drawn_samples


@st.cache_data
def display_briefing() -> None:
    st.title("Explanation Evaluation: Survey Instructions")
    
    st.markdown("""
    In this study, You will help us evaluate **visual explanations** generated by machine learning (ML) models.

    Modern ML models are often applied to image classification tasks. For instance, identifying whether an image shows a dog, a cat, or a car. While these models are often accurate, understanding *why* a model made a certain prediction is critical for building trust and improving transparency.

    That's where **explanation methods** come into play. These methods highlight image regions that were most important for the model's decision. In this survey, Your task is to assess how well these highlighted regions align with the actual object in the image.

    **Goal**: We want to understand how well explanations align with the **ground truth** (i.e., the real object in the image) and if explanations also highlight sotmethin other than the **ground truth**, if that is helpful for understanding its decision. 
    
    Your responses provide a human perspective that goes beyond what automated metrics can capture.

    ---
    """)

    st.subheader("How to Rate Explanations")

    st.write("""
    For each explanation, You will see an image with a **green border** around the classified object - the **ground truth**. Next to it, You will find an **importance map** that highlights the regions the model considered important for its classification.
    You will rate each explanation on two scales:
    
    **Alignment with Ground Truth**: To what degree does the importance map align with the classified object bordered in green?
    In simpler terms, does the highlighted region give You the impression, that the model is focusing on the **ground truth**?
    
    **Relevance of Explanation**: Does the importance map not related to the **ground truth** appear meaningful for classifying the image?
    In simpler terms, if the explanation highlights parts of the image that are not related to the **ground truth** object, does it still provide useful information for understanding the model's decision?
    Both questions use a scale from **1 (poor) to 5 (perfect)**.
    For the second question, You can also select "The importance map is perfectly aligned" if You perceive the explanation to be entirely focused on the **ground truth**.
    """)

    st.subheader("Example Evaluations")
    
    """
    In the following examples, You will see multiple different explanations for two images, their corresponding ground truth, and some comments from our side.
    We want them to give You the following intuition:
    The shape/ style of different importance maps depends heavily on the method.
    As a consequence, an ideal importance map looks by nature different for each method. 
    There are cases, where two methods can perfecty explain the model, yet their importance maps look very different in shape/ style.
    
    Make Yourself familiar with different methods and their resulting explanation shape/ style by looking at the examples below. Afterwards, proceed to the survey."""
    
    st.divider() # Add a divider for better separation
    
    # Display the examples
    for _, images in EXAMPLE_IMAGES.items():
        for exp_path, comment in images:
            st.image(Image.open(exp_path), use_container_width=True)
            st.write(f'{comment}')
        st.divider() # Add a divider for better separation

db = get_database()
users_collection = db['users']
remaining_samples = db['combinations']
familiarities = db['familiarities']
responses = db['responses']
manipulation_checks = db['manipulation_checks']
manipulation_reports = db['manipulation_reports']

if 'logged_in' not in st.session_state:
    st.session_state.logged_in = False
    
# Generate a unique user ID when the survey starts
if 'user_id' not in st.session_state:
    while True:
        user_id = str(uuid.uuid4())  # Generate a new UUID
        if not responses.find_one({'user_id': user_id}):  # Check if the user ID already exists
            st.session_state.user_id = user_id
            break
        
# Update session state initialization
if 'sampled_explanations' not in st.session_state:
    st.session_state.sampled_explanations = []
    drawn_samples = draw_samples(NUM_SAMPLES, remaining_samples)
    sampled_explanations = []
    for drawn_sample in drawn_samples:
        sample_folder = drawn_sample['sample']
        method = drawn_sample['method']
        threshold = drawn_sample['threshold']
        sampled_explanations.append({
            'type': 'xai',
            'object_folder': sample_folder,
            'method': method,
            'threshold': threshold
        })
        print(f"Sampled: {sample_folder}, {method}, {threshold}")
        
    attention_checks = random.sample(ATTENTION_CHECKS, k=NUM_CHECKS)
    for check in attention_checks:
        insert_index = random.randint(0, len(sampled_explanations))
        sampled_explanations.insert(insert_index, check)
    st.session_state.sampled_explanations = sampled_explanations
        
    
# Initialize session state variables
if 'evaluation_started' not in st.session_state:
    st.session_state.evaluation_started = False
if 'examples_shown' not in st.session_state:
    st.session_state.examples_shown = False
if 'current_index' not in st.session_state:
    st.session_state.current_index = 0
# if 'responses' not in st.session_state:
#     st.session_state.responses = []
if 'manipulation_checks' not in st.session_state:
    st.session_state.manipulation_checks = []
if 'ml_familiarity' not in st.session_state:
    st.session_state.ml_familiarity = None
if 'show_warning' not in st.session_state:
    st.session_state.show_warning = False  # Flag for warning visibility

if not st.session_state.logged_in:
    st.title("Login")  # Login UI
    st.session_state.username = st.text_input("Username")
    st.session_state.password = st.text_input("Password", type="password")
    if st.button("Login"):
        if login(st.session_state.username, st.session_state.password):  # Validate credentials
            st.session_state.logged_in = True
            st.success("Login successful!")
            st.rerun()
        else:
            st.error("Invalid username or password.")  # Error message for invalid credentials
        st.stop()
        
# Show examples before starting evaluation
elif not st.session_state.examples_shown:
    
    display_briefing()
    
    if st.button('Proceed to Survey'):
        st.session_state.examples_shown = True
        st.rerun()

# Initial question
elif not st.session_state.evaluation_started:
    familiarity_map = QUESTION_SCALE_MAP['familarity']
    
    # Set a default value only if not already set
    if 'ml_familiarity' not in st.session_state:
        st.session_state.ml_familiarity = None

    # Bind radio selection directly to session state
    selected_familiarity = st.radio(
        familiarity_map['question'],
        familiarity_map['scale'], 
        index=None,
        horizontal=True
    )
    
    increase_font_size() 
    
    if st.button('Start Evaluation'):
        if selected_familiarity is not None:
            st.session_state.evaluation_started = True
            st.session_state.ml_familiarity = selected_familiarity
            st.session_state.timestamp = datetime.now().isoformat()
            familiarity = {
                'user_group': st.session_state.username,
                'user_id': st.session_state.user_id,
                'ml_familiarity': st.session_state.ml_familiarity,
                'timestamp': st.session_state.timestamp
            }
            familiarities.insert_one(familiarity)
            st.rerun()
        else:
            st.warning('Please select an answer before proceeding.')

else:
    # Display images and questions
    if st.session_state.current_index < len(st.session_state.sampled_explanations):
        # Get the current drawn_sample
        drawn_sample = st.session_state.sampled_explanations[st.session_state.current_index]
        
        if drawn_sample.get('type') != 'xai':
            if drawn_sample.get('type') == 'manipulation':
                st.markdown("**Please indicate your agreement with the statements below**")
                answer = st.radio(
                    drawn_sample['question'],
                    ["Strongly Disagree", "Disagree", "Agree", "Strongly Agree"],
                    index=None,
                    key=f"manipulation_{st.session_state.current_index}",
                    horizontal=True
                )

            elif drawn_sample.get('type') == 'attention':
                st.markdown(f"**{drawn_sample['question']}**")
                answer = st.radio(
                    "Based on the text you read above, what colour have you been asked to enter?",
                    ['Red', 'Blue', 'Green', 'Orange', 'Brown'],
                    index=None,
                    key=f"attention_{st.session_state.current_index}"
                )
            
            if st.button('Next'):
                if answer is None:
                    st.session_state.show_warning = True
                    st.rerun()
                # Save the responses
                manipulation_check = {
                    'user_id': st.session_state.user_id,
                    'question': drawn_sample['question'],
                    'answer': answer,
                    'failed': 0 if answer in drawn_sample.get('correct_answers') else 1,
                    'index': st.session_state.current_index,
                    'timestamp': datetime.now().isoformat()
                }
                
                manipulation_checks.insert_one(manipulation_check)

                st.session_state.manipulation_checks.append(manipulation_check)
                st.session_state.current_index += 1
                st.session_state.show_warning = False  # Reset warning when user proceeds
                st.rerun()
        else:
            object_folder = drawn_sample['object_folder']
            method = drawn_sample['method']
            threshold = drawn_sample['threshold']

            explanation_path = os.path.join(IMAGE_FOLDER, object_folder, method, threshold)
            st.image(Image.open(explanation_path), use_container_width=True)

            st.divider() # Add a divider for better separation
            
            # Ask the alignment question and check for a valid response
            alignment = None
            alignment_map = QUESTION_SCALE_MAP['alignment']
            alignment = st.radio(
                alignment_map['question'],
                alignment_map['scale'],
                index=None, 
                key=f'xai_alignment_{st.session_state.current_index}',
                horizontal=True
            )
            
            st.divider() # Add a divider for better separation
            
            # Ask the relevance question and check for a valid response
            relevance = None
            relevance_map = QUESTION_SCALE_MAP['relevance']
            relevance = st.radio(
                relevance_map['question'],
                relevance_map['scale'],
                index=None, 
                key=f'xai_relevance_{st.session_state.current_index}',
                horizontal=True
            )
            
            increase_font_size()
            st.divider() # Add a divider for better separation
            
            # Display warning if no response is selected
            if st.session_state.show_warning:
                st.warning('Please select answers to continue.')
                
            if st.button('Next'):
                if alignment is None or relevance is None:
                    st.warning('Please select an answer before proceeding.')
                    st.rerun()
                # Save the responses
                response = {
                    'user_group': st.session_state.username,
                    'user_id': st.session_state.user_id,
                    'sample': object_folder,
                    'method': method,
                    'threshold': threshold.split('.')[0],
                    'alignment': alignment,
                    'relevance': relevance,
                    'timestamp': datetime.now().isoformat()
                }

                responses.insert_one(response)

                # Delete the evaluated drawn_sample from the collection
                remaining_samples.delete_one({
                    'sample': object_folder,
                    'method': method,
                    'threshold': threshold
                })
            
                st.session_state.current_index += 1
                st.session_state.show_warning = False  # Reset warning when user proceeds
                st.rerun()
    else:
        number_checks_failed = 0
        for manipulation_check in st.session_state.manipulation_checks:
            # Insert each manipulation check into the database
            number_checks_failed += manipulation_check.get('failed')
        
        manipulation_reports.insert_one({
            'user_group': st.session_state.username,
            'user_id': st.session_state.user_id,
            'indices': [mc['index'] for mc in st.session_state.manipulation_checks],
            'number_checks': len(st.session_state.manipulation_checks),
            'number_checks_failed': number_checks_failed
        })
        
        st.success('Evaluation completed! Results sent to MongoDB.')